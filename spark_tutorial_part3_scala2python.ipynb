{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: From Scala to Python\n",
    "\n",
    "Author: **Julien Peloton** [@JulienPeloton](https://github.com/astrolabsoftware/spark-tutorials/issues/new?body=@JulienPeloton)  \n",
    "Last Verifed to Run: **2018-10-30**  \n",
    "\n",
    "The core of Apache Spark is written is [Scala](https://scala-lang.org/), a general-purpose programming language that has been started in 2004 by Martin Odersky (EPFL). The language is inter-operable with Java and Java-like languages, and Scala executables run on the Java Virtual Machine (JVM). Like Java it has a strong static type system, but it does not require type declarations. Finally, Scala being younger than Java (1995), its design tries to solve some of the problems of Java.\n",
    "\n",
    "Scala is a portmanteau of scalable and language, to reflect the fact that the language is designed to grow and easily integrate new features over time. There are pros and cons to this... For example it is easy to extend or add specific features from other languages, but then there is no guarantee for backward compatibility between Scala versions. In addition, the syntax of the language is not unique (many optional rules, like type hints).\n",
    "\n",
    "Scala is not a pure functional programming language. It is multi-paradigm, including functional programming, imperative programming, object-oriented programming and concurrent computing. For those interested in Scala, you can follow this quick (~40min) introductory tutorial: [scala-tutorials](https://github.com/astrolabsoftware/scala-tutorials).\n",
    "\n",
    "Why am I talking about Scala in a PySpark context? Because you cannot ignore it! While PySpark is nowadays more than just an interface around Spark, most of it still comes straight from Spark (in Scala). To get the best of PySpark you need to know a few functional programming concepts present in Scala.\n",
    "\n",
    "__Learning objectives__\n",
    "\n",
    "- Strict vs non-strict (lazy) evaluation\n",
    "- Transformations vs actions\n",
    "- User Defined Functions\n",
    "- Cache or not cache? That's the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strict vs non-strict (lazy) evaluation\n",
    "\n",
    "Let's take a simple example (adapted from M. Odersky):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Initialise a RDD\n",
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "# xs is f(data)\n",
    "xs = rdd.map(lambda x: ...)\n",
    "\n",
    "# Action 1: filter some elements of xs and sum them\n",
    "xs.filter(...).sum()\n",
    "\n",
    "# Action 2: Take n elements of xs\n",
    "xs.take(n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a strict language, `map` is evaluated once and we produce intermediate list `xs`. In the case of a lazy language, `map` is evaluated twice and there is no intermediate list.\n",
    "\n",
    "In other words strict evaluates expressions as soon as they are available, rather than as needed. Lazy waits for the whole chain of actions before taking a decision on when the evaluation should happen.\n",
    "\n",
    "Spark collections however are **lazy** by default. Scala collections are **strict** by default.\n",
    "\n",
    "### What does it imply in the case of Apache Spark (i.e. lazy)?\n",
    "\n",
    "- You can define thousands intermediate objects using _transformations_ (`map` is one, see below for more information on action vs transformation), the computation will not be triggered as long as you do not call an action at the end.\n",
    "- Once you have define you chain of transformations plus the action at the end, Spark will analyse it, create a DAG (direct acyclic graph), and often optimize much better than we could do (as humans).\n",
    "- A long chain of operations on a RDD will be evaluated each time you call an action at the end. No panic, to avoid re-processing everything from the initial RDD you have also the possibility of _caching_ intermediate results. This is discussed in the last section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations vs actions\n",
    "\n",
    "There is a clear distinction between [transformation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) and [action](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions): a transformation creates a new dataset from an existing one, and an action returns a value to the driver program after running a computation on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "\n",
    "# Transformation: create a new rdd with twice\n",
    "# the initial value. Nothing really happens in \n",
    "# terms of IO or computation.\n",
    "rddp = rdd.map(lambda x: 2 * x)\n",
    "\n",
    "# Action: compute the sum and return it.\n",
    "# Computation is triggered.\n",
    "# note: we could have use the sum() method\n",
    "# rddp.sum()\n",
    "rddp.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we manipulate the dataset by applying transformations onto it (`map`, `filter`, `union`, ...) and only when the data exploration has reduced most of the data set size, we perform an action (`count`, `reduce`, `collect`, `sum`, `take`...) which triggers the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data: lazy!\n",
    "\n",
    "In the previous part (2), we dicussed the IO part of Spark. You probably did not notice, but this is a lazy operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No data transfer or work have been performed on the cluster, \n",
    "# since no actions have been called yet.\n",
    "df = spark.read.format('parquet').load('data/points.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation will be triggered only if you call an action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(x=0.9992780089378357), Row(x=0.9998085498809814), Row(x=0.9999639987945557), Row(x=0.9999489784240723), Row(x=0.9999311566352844), Row(x=0.9999566078186035), Row(x=0.9994782209396362), Row(x=0.9991017580032349), Row(x=0.999708354473114), Row(x=0.9999779462814331), Row(x=0.9993855953216553), Row(x=0.9997941851615906), Row(x=0.9991426467895508), Row(x=0.9990198612213135), Row(x=0.9999620318412781), Row(x=0.9998014569282532), Row(x=0.9995845556259155), Row(x=0.9998140335083008), Row(x=0.9997224807739258), Row(x=0.9993610978126526), Row(x=0.9998552799224854), Row(x=0.9995030164718628), Row(x=0.9993981122970581), Row(x=0.9993990063667297), Row(x=0.9996508955955505), Row(x=0.9996503591537476), Row(x=0.9992586374282837), Row(x=0.9995623826980591), Row(x=0.9995316863059998), Row(x=0.999855637550354), Row(x=0.999435544013977), Row(x=0.9994217753410339), Row(x=0.9996558427810669), Row(x=0.9994787573814392), Row(x=0.9997873902320862), Row(x=0.9998015761375427), Row(x=0.9995604753494263), Row(x=0.9999187588691711), Row(x=0.9993169903755188), Row(x=0.9997177720069885), Row(x=0.9996615052223206), Row(x=0.9992884993553162), Row(x=0.9991245865821838), Row(x=0.9991090893745422), Row(x=0.9998337626457214), Row(x=0.9990489482879639), Row(x=0.9999505877494812), Row(x=0.9995134472846985), Row(x=0.9998381733894348), Row(x=0.9994776844978333), Row(x=0.999485969543457), Row(x=0.9998655915260315), Row(x=0.9999682307243347), Row(x=0.9995340704917908)]\n",
      "+------------------+--------------------+--------------------+\n",
      "|                 x|                   y|                   z|\n",
      "+------------------+--------------------+--------------------+\n",
      "|0.9992780089378357|  0.3973689079284668| 0.49187254905700684|\n",
      "|0.9998085498809814| 0.07991381734609604| 0.11344659328460693|\n",
      "|0.9999639987945557|  0.1788404881954193| 0.09706342220306396|\n",
      "|0.9999489784240723|  0.6809957027435303|  0.6100340485572815|\n",
      "|0.9999311566352844| 0.14191728830337524|  0.4851420223712921|\n",
      "|0.9999566078186035|  0.5630964040756226|   0.942252516746521|\n",
      "|0.9994782209396362|  0.2988295257091522|   0.745749831199646|\n",
      "|0.9991017580032349|   0.759873628616333|  0.3763897716999054|\n",
      "| 0.999708354473114|  0.3914121687412262|  0.5763131380081177|\n",
      "|0.9999779462814331| 0.13316211104393005|  0.9535465836524963|\n",
      "|0.9993855953216553|  0.3035309612751007|   0.254142165184021|\n",
      "|0.9997941851615906|  0.6139796376228333| 0.13927225768566132|\n",
      "|0.9991426467895508| 0.06957888603210449| 0.30329906940460205|\n",
      "|0.9990198612213135| 0.13195817172527313|  0.6078134179115295|\n",
      "|0.9999620318412781|  0.9050752520561218| 0.38483721017837524|\n",
      "|0.9998014569282532|  0.7400572299957275|  0.9826500415802002|\n",
      "|0.9995845556259155|  0.7277294397354126|  0.7982686161994934|\n",
      "|0.9998140335083008|  0.5677172541618347|   0.954499363899231|\n",
      "|0.9997224807739258| 0.43521422147750854|0.051787082105875015|\n",
      "|0.9993610978126526|0.044430091977119446|  0.3497411012649536|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just another transformation\n",
    "df_filt = df.filter(df[\"x\"] > 0.999)\n",
    "\n",
    "# Trigger a proper action now\n",
    "print(df_filt.select(\"x\").collect())\n",
    "\n",
    "# Show is special in the sense that it collects\n",
    "# only a subset of the full dataset and print it.\n",
    "# That's why it is much faster than other actions\n",
    "# in general.\n",
    "df_filt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD vs DataFrame: the map case\n",
    "\n",
    "A DataFrame being based on a RDD, the kind of transformation you can apply onto it is similar. Most of transformations are available for both RDD and DataFrame and the syntax is quasi-identical (`filter`, `union`, etc...). The most notable difference is `map` which is not a DataFrame method. Let's investigate a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|         x|         y|        z|\n",
      "+----------+----------+---------+\n",
      "| 0.5488135|0.30764535|0.5352571|\n",
      "|0.71518934|0.06200521|0.9040443|\n",
      "+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial DataFrame\n",
    "df = spark.read.format('csv')\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"header\", True)\\\n",
    "    .load('data/points.csv')\n",
    "\n",
    "# Corresponding RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Data\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there are plenty of ways to create a DataFrame from a RDD or a collection. For more information, see for example `spark.createDataFrame?`. Let's now multiply all `x` entries by 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.097627, 0.30764535, 0.5352571]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDDs are simple\n",
    "rdd.map(lambda arow: [2 * arow[0], arow[1], arow[2]]).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+\n",
      "|         x|         y|        z|\n",
      "+----------+----------+---------+\n",
      "|  1.097627|0.30764535|0.5352571|\n",
      "|1.43037868|0.06200521|0.9040443|\n",
      "+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+---------+\n",
      "|         x|         y|        z|\n",
      "+----------+----------+---------+\n",
      "|  1.097627|0.30764535|0.5352571|\n",
      "|1.43037868|0.06200521|0.9040443|\n",
      "+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+---------+\n",
      "|         x|         y|        z|\n",
      "+----------+----------+---------+\n",
      "|  1.097627|0.30764535|0.5352571|\n",
      "|1.43037868|0.06200521|0.9040443|\n",
      "+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+----------+---------+\n",
      "|         x|         y|        z|\n",
      "+----------+----------+---------+\n",
      "|  1.097627|0.30764535|0.5352571|\n",
      "|1.43037868|0.06200521|0.9040443|\n",
      "+----------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame are slightly more convoluted...\n",
    "\n",
    "#############################\n",
    "# Method 1: Go back to RDD...\n",
    "df.rdd\\\n",
    "    .map(lambda arow: [2 * arow[0], arow[1], arow[2]])\\\n",
    "    .toDF(df.columns)\\\n",
    "    .show(2)\n",
    "\n",
    "#############################\n",
    "# Method 2: manipulate entire column at once\n",
    "from pyspark.sql.functions import col\n",
    "df.withColumn(\"x\", 2 * col(\"x\")).show(2)\n",
    "\n",
    "#############################\n",
    "# Method 3: à la SQL\n",
    "# SQL - register first the DataFrame\n",
    "df.createOrReplaceTempView(\"myDF\")\n",
    "\n",
    "# Define your command\n",
    "sql_command = \"\"\"\n",
    "    SELECT 2 * x AS x, y, z\n",
    "    FROM myDF \n",
    "\"\"\"\n",
    "\n",
    "# Execute the expression - return a DataFrame\n",
    "df_mult = spark.sql(sql_command)\n",
    "df_mult.show(2)\n",
    "\n",
    "#############################\n",
    "# Method 4: User Defined function\n",
    "# We will explore this in the next section\n",
    "from pyspark.sql import functions as F\n",
    "mult = F.udf(lambda x: 2 * x)\n",
    "df.select(mult(df[\"x\"]).alias(\"x\"), \"y\", \"z\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general (not just for map), DataFrames have several way for doing the same thing. Example on `filter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44874 = 44874 = 44874\n"
     ]
    }
   ],
   "source": [
    "# Filter data\n",
    "a = df.filter(df[\"x\"] > 0.1).count()\n",
    "b = df.where(df[\"x\"] > 0.1).count()\n",
    "c = spark.sql(\"SELECT x FROM myDF WHERE x > 0.1\").count()\n",
    "print(\"{} = {} = {}\".format(a, b, c))\n",
    "# and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very much in the spirit of Scala... ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions\n",
    "\n",
    "The previous difference in behaviour brings us to... User Defined Functions! Although there are many simple functions accessible from the Spark SQL module, you ultimately want to execute aribtrary code on your DataFrame data. One way to do so is via User Defined Functions (UDF). \n",
    "\n",
    "Let's face the reality: there is a problem of performance with UDF in Python compared to their counterpart in Scala. It is discussed in length in [1807.03078](https://arxiv.org/abs/1807.03078) for example. If you write down a simple UDF and apply it to your dataset, you might be 100 times slower than with the same code in Scala. Oups... \n",
    "\n",
    "The reasons are rather technical, mainly in the way Python interacts with the JVM + the way Python processes work. Keep in mind PySpark is a Spark Python API which is more or less an interface for Spark Scala in Python via py4j.\n",
    "\n",
    "In order to solve this problem, recent Spark version introduces vectorised UDF (or called pandas UDF). Once used, the difference with the Scala counterpart is less (but still non-negligible). See for example this simple explanation [here](https://databricks.com/session/keynote-from-reynold-xin) (from 7.50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ms ± 2.01 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "50.3 ms ± 6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "\n",
    "# Standard version\n",
    "mult = F.udf(lambda x: float(np.cos(x**2) * np.sin(x**3)))\n",
    "\n",
    "# Vectorized version\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def multpd(x):\n",
    "    \"\"\" Compute something...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Column\n",
    "        One-dimensional ndarray. This is typically\n",
    "        a DataFrame column (df[\"name\"]).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        pandas.Series: One-dimensional ndarray.\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = np.cos(x**2) * np.sin(x**3)\n",
    "    return pd.Series(ret)\n",
    "\n",
    "# Initial DataFrame\n",
    "df = spark.read.format('fits')\\\n",
    "    .option(\"hdu\", 1)\\\n",
    "    .load('data/points.fits')\\\n",
    "    .cache()\n",
    "\n",
    "# Note that given the small dataset, and \n",
    "# the rather simple computation, you might not\n",
    "# notice the time difference depending on your machine...\n",
    "%timeit df.select(mult(df[\"x\"]).alias(\"x\")).count()\n",
    "%timeit df.select(multpd(df[\"x\"]).alias(\"x\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can pass several columns to UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             x + y|\n",
      "+------------------+\n",
      "|0.8564589023590088|\n",
      "|0.7771945595741272|\n",
      "|1.5915484428405762|\n",
      "+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorized version\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def add_cols(x, y):\n",
    "    \"\"\" Add two columns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : One-dimensional ndarray\n",
    "        This is typically a DataFrame column (df[\"name\"]).\n",
    "    y : One-dimensional ndarray\n",
    "        This is typically a DataFrame column (df[\"othername\"]).\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "        pandas.Series: One-dimensional ndarray.\n",
    "    \n",
    "    \"\"\"\n",
    "    ret = x + y\n",
    "    return pd.Series(ret)\n",
    "\n",
    "df.select(add_cols(df[\"x\"], df[\"y\"]).alias(\"x + y\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache or not cache? That's the question.\n",
    "\n",
    "You might notice that we used the method `cache()` earlier. What is it?\n",
    "\n",
    "Spark is based on the so-called MapReduce cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance. But prior to Spark, in Hadoop for example, MapReduce was really Map-Reduce-Map-Reduce-Map-... That is data was loaded, manipulated once, dumped to disk, reloaded, re-manipulated once, etc... This is not super efficient if you want to perform iterative steps or re-use the same input many times in different context.\n",
    "\n",
    "Although Spark is not an in-memory technology per se (Spark has pluggable connectors for different persistent storage systems but it does not have native persistence code), it allows users to efficiently use in-memory Least Recently Used (LRU) cache relying on disk only when the allocated memory is not sufficient. Ideally, the data will be loaded from disk into partitions only the first time and the totality or some part of it will be kept in-memory (distributed among processors), such that the subsequent data explorations will be limited by the computation time only. If the dataset size is bigger than the total available cache memory in the cluster, the user needs to decide whether remaining partitions not cached will be spill to disk at the first iteration or recomputed from scratch later if needed.\n",
    "\n",
    "For more information, including which storage level to choose, see http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,\n",
      "    whether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\n",
      "    in a JAVA-specific serialized format, and whether to replicate the RDD partitions on multiple\n",
      "    nodes. Also contains static constants for some commonly used storage levels, MEMORY_ONLY.\n",
      "    Since the data is always serialized on the Python side, all the constants use the serialized\n",
      "    formats.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# StorageLevel gives you control of the kind\n",
    "# of partition storage you want to use.\n",
    "print(StorageLevel.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Initial DataFrame - nothing has been\n",
    "# assumed about the cache, i.e. no cache.\n",
    "df = spark.read.format('fits')\\\n",
    "    .option(\"hdu\", 1)\\\n",
    "    .load('data/points.fits')\n",
    "print(df.is_cached)\n",
    "\n",
    "# Decide to cache entirely the DF\n",
    "# Make sure you have enough memory available\n",
    "# df.cache() is a shorthand for df.persist(StorageLevel.MEMORY_ONLY)\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(df.is_cached)\n",
    "\n",
    "# Manually unpersist data. Not mandatory as Spark\n",
    "# automatically monitors cache usage on each node and \n",
    "# drops out old data partitions in a Least Recently Used (LRU) fashion.\n",
    "df.unpersist()\n",
    "print(df.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check, even on this very small dataset, the benefit of caching the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.2 ms ± 3.76 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "31.6 ms ± 2.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Just in case...\n",
    "df.unpersist()\n",
    "%timeit df.count()\n",
    "\n",
    "# Cache it\n",
    "df_cached = df.cache()\n",
    "df_cached.count()\n",
    "%timeit df_cached.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a common mistake is to think that all the CPU RAM can be used for the cache. It is wrong. The default in Spark sets the total memory fraction dedicated to the caching to 60% of the total RAM. Why? Because you need the remaining for other things (JVM, computation, shuffle, ...) ;-)\n",
    "So if you have a cluster with 1000 CPU @ 2 GB RAM each, you will be able to cache up to 1.2 TB max (and not 2 TB!). As usual in Spark, this parameter is tunable either via configuration file or directly in command line when launching your job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "Here is a series of useful links on similar topics:\n",
    "\n",
    "- Doc on Apache Spark [transformation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) and [action](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "- Apache Spark and data persistence: http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence\n",
    "- Talks on (vectorised) PySpark UDF: [here](https://databricks.com/session/vectorized-udf-scalable-analysis-with-python-and-pyspark) and [there](https://databricks.com/session/making-pyspark-amazing-from-faster-udfs-to-dependency-management-graphing) for example.\n",
    "- Tutorials for Scala: https://github.com/astrolabsoftware/scala-tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
