{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Spark SQL and DataFrames\n",
    "\n",
    "Author: **Julien Peloton** [@JulienPeloton](https://github.com/astrolabsoftware/spark-tutorials/issues/new?body=@JulienPeloton)  \n",
    "Last Verifed to Run: **2018-10-29**  \n",
    "\n",
    "__Learning objectives__\n",
    "\n",
    "- Apache Spark Data Sources.\n",
    "- Loading and distributing data: Spark SQL and DataFrames.\n",
    "- Hands-on: RDD vs DataFrame, partitioning, limits, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Data Sources\n",
    "\n",
    "### A tour of data formats\n",
    "\n",
    "There are many data formats used in the context of Big Data: CSV (1978), XML (1996), JSON (2001), Thrift (2007), Protobuf (2008), Avro & SequenceFile (2009), Parquet (2013), ORC (2016), and the list goes on... Some are _naively_ structured that is using a single type to describe the data (e.g. text) without any internal organisation to access faster the data. Others are more complex and highly optimised for big data treatment (e.g. Parquet). Unfortunately those are not the data formats typically chosen by the scientific community. In astronomy you would rather store the data in FITS (1981) or HDF5 (1988) format. \n",
    "FITS and HDF5 are multi-purposes data formats: images, spectra, photon lists, data cubes, or even structured data such as multi-table databases can be efficiently stored and accessed.\n",
    "\n",
    "### Connecting to Data Source\n",
    "\n",
    "The data source API in Apache Spark belongs to the [Spark SQL module](https://spark.apache.org/sql/). Note that Spark Core has some simple built-in ways to read data from disk (binary or text), but Spark SQL is more complete. If you want to connect a particular data source with Apache Spark, you have mostly two ways:\n",
    "\n",
    "- [indirect] Access and distribute your files as binary streams (Spark does it natively), and decode the data on-the-fly within executors using third-party libraries.\n",
    "- [native] Write a custom connector to access, distribute and decode the data natively.\n",
    "\n",
    "FITS or HDF5 as most of scientific data formats, were not designed for serialisation (distribution of data over machines) originally and they often use compression to reduce the size on disk. Needless to say that default Spark cannot read those natively. \n",
    "\n",
    "First attempts to connect those data formats (see e.g. [1] for FITS) with Spark were using the indirect method above. By reading files as binary streams, the indirect method has the advantage of having access to all FITS functionalities implemented in the underlying user library. This can be an advantage when working with the Python API for example which already contains many great scientific libraries. However this indirect method assumes each Spark mapper will receive and handle one entire file (since the filenames are parallelized and entire file data must be reconstructed from binary once the file has been opened by a Spark mapper). Therefore each single file must fit within the memory of a Spark mapper, hence the indirect method cannot distribute a dataset made of large FITS files (e.g. in [1] they have a 65 GB dataset made of 11,150 files). In addition by assuming each Spark mapper will receive and handle one entire file, the indirect method will have a poor load balancing if the dataset is made of files with not all the same size.\n",
    "\n",
    "Fortunately Apache Spark low-level layers are sufficiently well written to allow extending the framework and write native connectors for any kind of data sources. Recently connectors for FITS and HDF5 were made available [2, 3] to the community. With such connectors, there is a guarantee of having a good load balancing regardless the structure of the dataset and the size of the input files is no more a problem (a 1 TB dataset made of thousand 1 GB files or one single 1 TB file will be viewed as almost the same by a native Spark connector). Note however that the Data Source API is in Java/Scala and if there is no library to play with your data source in those languages you must implement it (what has been done in [2]) or interface with another language.\n",
    "\n",
    "Note that the low-level layers dealing with the data sources have been recently updated. Apache Spark 2.3 introduced the Data Source API version 2. While the version 1 is still available and usable for a long time, we expect that all Spark connectors will comply with this v2 in the future.\n",
    "\n",
    "[1] Z. Zhang and K. Barbary and F. A. Nothaft and E. R. Sparks and O. Zahn and M. J. Franklin and D. A. Patterson and S. Perlmutter, Kira: Processing Astronomy Imagery Using Big Data Technology, DOI 10.1109/TBDATA.2016.2599926.  \n",
    "[2] Peloton, Julien and Arnault, Christian and Plaszczynski, St√©phane, FITS Data Source for Apache Spark, Computing and Software for Big Science (1804.07501). https://github.com/astrolabsoftware/spark-fits   \n",
    "[3] Liu, Jialin and Racah, Evan and Koziol, Quincey and Canon, Richard Shane, H5spark: bridging the I/O gap between Spark and scientific data formats on HPC systems, Cray user group (2016). https://github.com/valiantljk/h5spark  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and distributing data: Spark SQL and DataFrames\n",
    "\n",
    "### DataFrameReader\n",
    "\n",
    "The interface to read data from disk is always the same for any kind of built-in and officially supported data format:\n",
    "\n",
    "```python\n",
    "df = spark.read\\\n",
    "    .format(format: str)\\\n",
    "    .option(key: str, value: Any)\\\n",
    "    # ...\n",
    "    .option(key: str, value: Any)\\\n",
    "    .load(path: str)\n",
    "```\n",
    " \n",
    "Note that for most of the data sources, you can use wrappers such as:\n",
    "\n",
    "```python\n",
    "spark.read.csv(path, key1=value1, key2=value2, ...)\n",
    "```\n",
    "\n",
    "**Format**: The format can be \"csv\", \"json\", \"parquet\", etc. \n",
    "\n",
    "**Options**: The number of options depends on the underlying data source. Each has its own set of options. \n",
    "In most of the case, no options are needed, but you might want to explore the different possibilities at some point. Surprisingly it is not easy to find documentation and the best remains to read the source code documentation. In pyspark you can easily access it via the wrappers (use IPython for the driver -- see part 1):\n",
    "\n",
    "```python\n",
    "# DataFrameReader object\n",
    "df_reader = spark.read\n",
    "\n",
    "# Doc on reading CSV\n",
    "df_reader.csv?\n",
    "# doc printed\n",
    "\n",
    "# Doc on reading Parquet\n",
    "df_reader.parquet?\n",
    "# doc printed\n",
    "```\n",
    "\n",
    "\n",
    "**Path**: The way to specify path is threefold: either a single file (`path/to/folder/myfile.source`), or an entire folder (`path/to/folder`), or a glob pattern (`path/to/folder/*pattern*.source`). Note that you also need to specify the type of file system you are using. Example:\n",
    "\n",
    "``` python\n",
    "# Connect to hdfs\n",
    "path = 'hdfs:///path/to/data'\n",
    "\n",
    "# Connect to S3\n",
    "path = 's3:///path/to/data'\n",
    "\n",
    "# Connect to local file system\n",
    "path = 'files:///path/to/data'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Using a custom connector\n",
    "\n",
    "You can also connect to custom connector not included in the default Spark distribution. To do so, you will need to specify the dependencies when submitting your job or invoking your shell (see part 1). If your connector is available through [Maven Central Repository](https://search.maven.org/), you can easily specify it via:\n",
    "\n",
    "```\n",
    "# Direct download from central repository\n",
    "spark-submit --packages groupId:artifactId:version ...\n",
    "```\n",
    "\n",
    "Note that this is the same syntax when launching the `pyspark` shell.\n",
    "For example, if you want to read FITS files using the [spark-fits](https://github.com/astrolabsoftware/spark-fits) connector you would add the following:\n",
    "\n",
    "```\n",
    "# Direct download from central repository\n",
    "spark-submit --packages com.github.astrolabsoftware:spark-fits_2.11:0.7.1 ...\n",
    "```\n",
    "\n",
    "You can find the spark-fits entry in the Maven Central [here](https://search.maven.org/artifact/com.github.astrolabsoftware/spark-fits_2.11/0.7.1/jar) for reference.\n",
    "Alternatively you can download the source code for a particular connector, compile it and include the `jars`:\n",
    "\n",
    "```\n",
    "# Specify manually the dependency\n",
    "spark-submit --jars /path/to/lib/spark-fits.jars ...\n",
    "```\n",
    "\n",
    "Note that when you launch `pyspark`, already a numbers of `jars` are included by default (the ones for Spark for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "You will find test data in the folder `spark-tutorials/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data: simply structured data (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load simple CSV file\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .load(\"data/simple.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice by default the CSV connector interpret all entries as String, and give dummy names to columns. You can infer the data type and use the first row as column names by specifying options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option('inferSchema', True)\\\n",
    "    .option('header', True)\\\n",
    "    .load(\"data/simple.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|toto|  3|\n",
      "|tutu|  4|\n",
      "|titi|  1|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a nice representation of our data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you could do the same with the RDD API, but you would need more steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using Spark RDD API\n",
    "rdd = sc.textFile(\"data/simple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements of the RDD are: ['name,age', 'toto,3']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Each row is a String with all \n",
    "# column value\n",
    "print(\"First two elements of the RDD are: {}\\n\".format(rdd.take(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First row contains column names\n",
    "names = rdd.map(lambda x: x.split(\",\")).take(1)[0]\n",
    "\n",
    "# Make it a DataFrame\n",
    "# 1. remove the first line which contains metadata\n",
    "# 2. split row elements into columns\n",
    "# 3. Assign correct data type\n",
    "# 4. Promote to DataFrame\n",
    "df = rdd\\\n",
    "    .filter(lambda x: 'name' not in x)\\\n",
    "    .map(lambda x: x.split(\",\"))\\\n",
    "    .map(lambda x: [x[0], int(x[1])])\\\n",
    "    .toDF(names)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data: complex structured data (Parquet)\n",
    "\n",
    "More complex data format can infer automatically schema, and data types.\n",
    "They are also optimised for fast data access and small memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|toto|  3|\n",
      "|tutu|  4|\n",
      "|titi|  1|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same using Parquet - Note that the schema and the data types \n",
    "# are directly inferred. (use long for int by default).\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"data/simple.parquet\")\n",
    "df_parquet.printSchema()\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame to RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can always go from a DataFrame back to the underlying RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='toto', age=3), Row(name='tutu', age=4), Row(name='titi', age=1)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_from_df = df_parquet.rdd\n",
    "rdd_from_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the returned type is a list of `Row`. `Row` is an internal Spark type. You can easily convert it to `dict`, `list` or `numpy.ndarray` at your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'name': 'toto', 'age': 3}\n",
      "<class 'list'> ['toto', 3]\n",
      "<class 'numpy.ndarray'> ['toto' '3']\n"
     ]
    }
   ],
   "source": [
    "arow = rdd_from_df.take(1)[0]\n",
    "\n",
    "# Conversion to dict\n",
    "adict = arow.asDict()\n",
    "print(type(adict), adict)\n",
    "\n",
    "# Conversion to list\n",
    "alist = list(arow)\n",
    "print(type(alist), alist)\n",
    "\n",
    "# Conversion to numpy.ndarray\n",
    "# Data types are not preserved by default\n",
    "import numpy as np\n",
    "anarray = np.array(arow)\n",
    "print(type(anarray), anarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Partition size, resources, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing existing collection: beware of the 2G points per partition!\n",
    "\n",
    "In practice, there are two ways to create RDDs: referencing a dataset in an external storage system (see above), or parallelizing an existing collection in your driver program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = range(100)\n",
    "rdd_collection = sc.parallelize(mylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it seems a good idea sometimes, you need to be aware that for historical reasons there is a limit on how many numbers can be stored inside one partition: `2**31 - 1`! This comes from the fact that Java Arrays are indexed by int values. Accessing array component beyond this limit will result in a compile-time error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "Here is a series of useful links on similar topics:\n",
    "\n",
    "- Spark SQL module: https://spark.apache.org/sql/\n",
    "- Spark SQL code on GitHub: https://github.com/apache/spark/tree/master/sql\n",
    "- Spark SQL doc: http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- Databricks Data Source documentation: https://docs.databricks.com/spark/latest/data-sources/index.html\n",
    "- Apache Spark Data Source V2 explained in video (Spark Summit 2018): https://databricks.com/session/apache-spark-data-source-v2\n",
    "- Introducing Apache Spark Data Sources API V2 (IBM): https://developer.ibm.com/code/2018/04/16/introducing-apache-spark-data-sources-api-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
