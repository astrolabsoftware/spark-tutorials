{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Installation and first steps\n",
    "\n",
    "Author: **Julien Peloton** [@JulienPeloton](https://github.com/astrolabsoftware/spark-tutorials/issues/new?body=@JulienPeloton)  \n",
    "Last Verifed to Run: **2018-10-25**  \n",
    "\n",
    "Welcome to the series of notebooks on Apache Spark! The main goal of this series is to get familiar with Apache Spark, and in particular its Python API called PySpark. \n",
    "\n",
    "__Learning objectives__\n",
    "\n",
    "- Apache Spark: what it is?\n",
    "- Installation @ HOME\n",
    "- Using the PySpark shell\n",
    "- Your first Spark program\n",
    "- Going further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark \n",
    "\n",
    "[Apache Spark](http://spark.apache.org/) is a cluster computing framework, that is a set of tools to perform computation on a network of many machines. Spark started in 2009 as a research project, and it had a huge success so far in the industry. It is based on the so-called MapReduce cluster computing paradigm, popularized by the Hadoop framework using implicit data parallelism and fault tolerance. \n",
    "\n",
    "The core of Spark is written in Scala which is a general-purpose programming language that has been started in 2004 by Martin Odersky (EPFL). The language is inter-operable with Java and Java-like languages, and Scala executables run on the Java Virtual Machine (JVM). Note that Scala is not a pure functional programming language. It is multi-paradigm, including functional programming, imperative programming, object-oriented programming and concurrent computing.\n",
    "\n",
    "Spark provides many functionalities exposed through Scala/Python/Java/R API (Scala being the most complete one). As far as DESC is concerned, I would advocate to use the Python API (called PySpark) for obvious reasons. But feel free to put your hands on Scala, it's worth it. For those interested, you can have a look at this [tutorial](https://gitlab.in2p3.fr/MaitresNageurs/QuatreNages/Scala) on Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation @ HOME\n",
    "\n",
    "You might want to install Apache Spark on your laptop, to prototype programs and perform local checks. The easiest way to do so is to [download](https://spark.apache.org/downloads.html) a pre-built version of Spark (take the latest one). Untar it, move it to the location you want, and update your path such that it can be found when you launch a job:\n",
    "\n",
    "```bash\n",
    "# Put those lines in your HOME/.bash_profile\n",
    "SPARK_HOME=/path/to/spark\n",
    "export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH\n",
    "```\n",
    "\n",
    "Latest version of Spark should run on Java 8+, but I recommend using it on Java 8. On macOS, to see the different java jdk installed on your machine: \n",
    "\n",
    "```\n",
    "/usr/libexec/java_home -V\n",
    "```\n",
    "\n",
    "If Java 8 is not present, download the JDK and set it using:\n",
    "\n",
    "```bash\n",
    "# Put this line in your HOME/.bash_profile, with the \n",
    "# version number you just downloaded. Example:\n",
    "export JAVA_HOME=`/usr/libexec/java_home -v 1.8.0_151`\n",
    "```\n",
    "\n",
    "Obviously you won't process massive data sets on your laptop, but it is sufficient for getting familiar with the API and learn what's under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the PySpark shell\n",
    "\n",
    "### Python/IPython shells\n",
    "\n",
    "To access the PySpark shell, just type `pyspark` in a terminal. You will be redirected to the standard python shell, augmented with spark environment and pre-loaded objects such as the `sparkContext` (`sc`) and the `sparkSession` (`spark`). Between you and me, the standard python shell is rather ugly and lacks of nice functionalities. If you really want to increase your productivity, you probably want to switch to IPython as the Python binary executable to use for PySpark (in driver only). Just type in your shell:\n",
    "\n",
    "```\n",
    "PYSPARK_DRIVER_PYTHON=ipython pyspark\n",
    "```\n",
    "And you should see (with your corresponding Spark, Python and IPython versions):\n",
    "\n",
    "```\n",
    "Python 3.7.0 (default, Jun 28 2018, 07:39:16)\n",
    "Type 'copyright', 'credits' or 'license' for more information\n",
    "IPython 7.0.1 -- An enhanced Interactive Python. Type '?' for help.\n",
    "2018-10-24 21:13:45 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.7.0 (default, Jun 28 2018 07:39:16)\n",
    "SparkSession available as 'spark'.\n",
    "\n",
    "In [1]:\n",
    "```\n",
    "\n",
    "If it complains about IPython not found but you know it is installed somewhere, just specify the whole path to it (to see it: `which ipython`). As said previously, you'll have your Spark environment loaded and few objects ready:\n",
    "\n",
    "```python\n",
    "In [1]: # Spark Session \n",
    "In [2]: spark\n",
    "Out[2]: <pyspark.sql.session.SparkSession at 0x10d8d6e80>\n",
    "    \n",
    "In [3]: # Spark Context \n",
    "In [4]: sc\n",
    "Out[4]: <SparkContext master=local[*] appName=PySparkShell>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying resources\n",
    "\n",
    "By default, if you just execute `pyspark`, you will run in _local_ mode, with very few options:\n",
    "\n",
    "```python\n",
    "In [1]: sc.getConf().getAll()\n",
    "Out[1]:\n",
    "[('spark.eventLog.enabled', 'true'),\n",
    " ('spark.driver.port', '51813'),\n",
    " ('spark.driver.host', '192.168.0.10'),\n",
    " ('spark.executor.id', 'driver'),\n",
    " ('spark.app.name', 'PySparkShell'),\n",
    " ('spark.app.id', 'local-1540409933299'),\n",
    " ('spark.sql.catalogImplementation', 'hive'),\n",
    " ('spark.rdd.compress', 'True'),\n",
    " ('spark.eventLog.dir',\n",
    "  '/Users/julien/Documents/workspace/lib/spark/logfiles'),\n",
    " ('spark.serializer.objectStreamReset', '100'),\n",
    " ('spark.master', 'local[*]'),\n",
    " ('spark.submit.deployMode', 'client'),\n",
    " ('spark.ui.showConsoleProgress', 'true'),\n",
    " ('spark.history.fs.logDirectory',\n",
    "  '/Users/julien/Documents/workspace/lib/spark/logfiles')]\n",
    "```\n",
    "\n",
    "While it is useful for debugging and testing, it does not contain many options and it is not super useful to work at scale. Instead you should specify the kind of resources you want for your Spark cluster. There are litteraly hundreds of options that can be set (see [here](https://spark.apache.org/docs/latest/configuration.html)), but here is a summary of the most useful ones to start:\n",
    "\n",
    "```\n",
    "# At home\n",
    "PYSPARK_DRIVER_PYTHON=ipython pyspark --master local[*] \\\n",
    "  --driver-memory 2g --executor-memory 2g \\\n",
    "  --jars ... --packages ... --py-files ...\n",
    "\n",
    "# On a cluster, or at NERSC on interactive nodes\n",
    "ncore_per_node = 32\n",
    "n_node = ...\n",
    "PYSPARK_DRIVER_PYTHON=ipython pyspark --master ${SPARKURL} \\\n",
    "  --driver-memory 64g --executor-memory 64g \\\n",
    "  --executor-cores ${ncore_per_node} --total-executor-cores ${ncore_per_node} * ${n_node} \\\n",
    "  --jars ... --packages ... --py-files ...\n",
    "```\n",
    "\n",
    "Note that if needed, options can be automatically set in a `conf` file stored under `/path/to/spark/conf/`.\n",
    "\n",
    "The options `--jars`, `--packages`, and `--py-files` are ways to specify third-party packages. `--jars` and `--packages` control Java/Scala/etc. third-party packages with Python API you would like to use, and `--py-files` controls other python modules. You might say:\n",
    "\n",
    "```\n",
    "Why do I need to specify other python modules if they can be found in my PYTHONPATH or PATH?\n",
    "```\n",
    "\n",
    "The answer is: because of distributed computing! It is correct that your _driver_, that is the machine where your program is shipped is probably aware of those modules, but the _executors_, that is the other machines of the network, they probably do not know those (unless you explicitly installed the modules in all the machines of the network).\n",
    "Of course, if you are in local mode (e.g. in your laptop) _driver_ and _executors_ are in the same machine (just different threads) so you do not see this problem. But in a real cluster, you need to connect all of this. Note that modules listed in `--py-files` will be basically broadcasted to the executors. \n",
    "We will see examples later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Jupyter\n",
    "\n",
    "You can use Apache Spark with Jupyter as you would do for IPython:\n",
    "\n",
    "```\n",
    "PYSPARK_DRIVER_PYTHON=jupyter-notebook pyspark <...resource...>\n",
    "```\n",
    "\n",
    "Then select a Python kernel, and done! For NERSC, you will need a specific kernel (see the second part of this series of tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first Spark program\n",
    "\n",
    "Interactive jobs are OK, but at some point you might want to switch to batch mode. One of the strength of (Py)Spark is that you do not need a different structure for your program if you are dealing with KB of TB of data. Here is a very simple PySpark program (can be found at [astrolabsoftware/tutorials](https://github.com/astrolabsoftware/spark-tutorials)):\n",
    "\n",
    "```python\n",
    "# in resources/part1_example.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (1) Initialise the Spark Session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# (2) Generate fake data on the driver\n",
    "mylist = [\n",
    "    [\"Julien\", 67], \n",
    "    [\"Ιουλιανός\", 32], \n",
    "    [\"Юлиан\", 89],\n",
    "    [\"尤利安\", 40]\n",
    "]\n",
    "\n",
    "# (3) Distribute it over the network\n",
    "rdd = spark.sparkContext.parallelize(mylist)\n",
    "\n",
    "# (4) Return the mean of ages:\n",
    "meanage = rdd.map(lambda x: x[1]).mean()\n",
    "print(\"Mean age is {}\".format(meanage))\n",
    "\n",
    "# (5) Return person whose age is below 60\n",
    "belowthreshold = rdd\\\n",
    "    .filter(lambda x: x[1] < 60)\\\n",
    "    .map(lambda x: x[0])\\\n",
    "    .collect()\n",
    "print(\"{} is/are below 60\".format(belowthreshold))\n",
    "\n",
    "# (6) Go from RDD to DataFrame\n",
    "df = rdd.toDF([\"Name\", \"Age\"])\n",
    "df.show()\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "Remarks (see numbers in the example above):\n",
    "\n",
    "- (1) Note that this is automatically done for you in the PySpark shell.\n",
    "- (2) Just for the sake of having something simple enough. In practive you do not generate data on the driver. You would read and distribute (massive) data from disk for example. The IO is discussed in the second part of this tutorial.\n",
    "- (3) A Resilient Distributed Datasets (**RDD**) is a partitioned collection of records across all machines. RDDs are distributed memory abstractions, fault-tolerant and immutable. This is a central object in Spark. In the latest Spark version, you would rather manipulate **DataFrames**, which are more or less RDD plus a schema of the data. Even if you will not manipulate RDD explicitly in the future, keep this in mind.\n",
    "- (4) (a) For all pairs, take only the second element (b) compute the mean within each partition and reduce all sub-means to the driver, and (c) the final mean is computed on the driver.\n",
    "- (5) Keep only persons whose age is below 60, and return only their name to the driver. Note that partitions are processed in parallel as long as there is enough resource (if not enough resources, partitions will queue until a Spark mapper becomes available). Note that Spark will try to enforce data locality as much as possible (i.e. send computation to worker nodes as close as possible to the DataNodes where the data are stored).\n",
    "- (6) You can easily create a DataFrame from a RDD. If the data is simple enough (like in this case), data types are inferred.\n",
    "\n",
    "And you would execute this example using for example:\n",
    "\n",
    "```\n",
    "spark-submit --master local[*] \\\n",
    "    --driver-memory 2g --executor-memory 2g \\\n",
    "    example.py\n",
    "```\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- Spark is really verbose... An overwhelming flow of message coming from the JVM and Spark (log4j), and from our program... Whether they are useful or not, that depends on what you want to do (learn/debug/prod/...) and your programming skills (do I understand what is written? And if yes, can I switch off some of them without missing an important piece of information?). You can set the level of verbosity for Spark by either using a conf file (see [here](http://spark.apache.org/docs/latest/configuration.html#configuring-logging)) or directly inside your program.\n",
    "- For such a simple example, the time to start Spark is clearly dominating with respect to the actual computation. It's time to move to bigger data volumes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "Here is a series of useful links on similar topics:\n",
    "\n",
    "- TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
